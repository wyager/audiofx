{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263a750a-808f-4c9c-af75-eac7febd4981",
   "metadata": {},
   "source": [
    "# Training Neural Networks to Emulate Audio Effects\n",
    "\n",
    "I'm gearing up to start a new job at an AI lab, working on RL systems, and I want to do a bit of hands-on NN work to gear up.\n",
    "\n",
    "Let's figure out how to train a model to emulate various audio effects, like distortion units or reverbs or whatever.\n",
    "\n",
    "I haven't read any literature on how to do this, so I'll probably do some stupid stuff here, but I find it's more educational to go in blind sometimes. Basically everything I know about NNs is just from discussions with my friend who works on neural compression codecs.\n",
    "\n",
    "I figure an architecture that might work pretty well is to have the previous `m` input samples, $[i_n, i_{n+1}, ..., i_{n+m-1}, i_{n+m}]$, and the previous `m-1` output samples, $[o_n, o_{n+1}, ..., o_{n+m-1}]$, and train the network to generate the next output sample, $o_{n+m}$\n",
    "\n",
    "```\n",
    "                     .-------.\n",
    "[m prev inputs]----->|  The  |\n",
    "[m-1 prev outputs]-->| Model |-->[next output]\n",
    "                     '-------'\n",
    "```\n",
    "\n",
    "This architecture seems good to me because:\n",
    "\n",
    "1. It's easy to generate examples of input data\n",
    "2. It gives the model enough context to (theoretically) recover information like \"what is the current phase of this oscillator\"\n",
    "\n",
    "\n",
    "As for how to actually represent the audio, some initial thoughts:\n",
    "\n",
    "1. You could just pass in and extract straight up $[-1,1]$ float audio into/out of the model.\n",
    "   1. The model will have to grow hardware to implement a threshold detector ADC or something, which seems kind of wasteful\n",
    "   2. This fails to accurately capture entropy density in human audio perception. A signal with peak amplitude 0.01 is often just as clear to humans as a signal with peak amplitude 1.0. Which brings us to a possible improvement\n",
    "2. Pre- and post-process the audio with some sort of compander, like a $\\mu$-law\n",
    "   1. This will probably help the model maintain perceptual accuracy across wide volume ranges\n",
    "3. Encode a binary representation of the audio. This feels to me like it will probably be difficult for the model to reason about.\n",
    "4. Use some sort of one-hot encoding for the amplitude, probably in combination with a companding algorithm. This might be too expensive on the input side (since we have a lot of inputs), but could work well on the output side. We could have 256 different outputs for 8-bit audio, for example.\n",
    "   1. We could also possibly allow the network to output a continuous \"residual\" value for adding additional accuracy on top of the one-hot value.\n",
    "  \n",
    "For now, let's just try the first thing and see how it goes. I expect it will work OK for well-normalized (loud) audio and poorly for quiet audio.\n",
    "\n",
    "As for our loss function: probably some sort of perceptual similarity metric would be best, but let's start with a super simple metric like squared error in $\\mu$-law space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8cb6e-486e-43a0-9935-4c501337382f",
   "metadata": {},
   "source": [
    "First up, I need a bunch of training data. I took a bunch of songs from my music library and put them in the `examples` folder. (I'm old enought that I have about 75GB of local music files.) \n",
    "\n",
    "When we want to train a new effect, we'll have a bunch of training runs of the form:\n",
    "\n",
    "1. Pick a bunch of random song slices from the library\n",
    "2. Feed the song slices through the reference implementation of the effect (could be software or hardware)\n",
    "3. Train the model to predict the next sample of the effect across all I/O examples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e73757-ce9d-47a5-b8e7-c416baae255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def list_audio_files(examples_dir: str = \"examples\") -> List[str]:\n",
    "    \"\"\"\n",
    "    List all audio files in the examples directory.\n",
    "    \n",
    "    Args:\n",
    "        examples_dir: Path to the examples directory\n",
    "        \n",
    "    Returns:\n",
    "        List of audio file paths\n",
    "    \"\"\"\n",
    "    audio_extensions = {'.mp3', '.m4a', '.wav', '.flac', '.aac', '.ogg', '.mp4'}\n",
    "    audio_files = []\n",
    "    \n",
    "    examples_path = Path(examples_dir)\n",
    "    if not examples_path.exists():\n",
    "        raise FileNotFoundError(f\"Examples directory '{examples_dir}' not found\")\n",
    "    \n",
    "    for file_path in examples_path.rglob('*'):\n",
    "        if file_path.is_file() and file_path.suffix.lower() in audio_extensions:\n",
    "            audio_files.append(str(file_path))\n",
    "    \n",
    "    return sorted(audio_files)\n",
    "\n",
    "def read_audio_normalized(file_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read an audio file, convert to mono, and normalize so peak absolute value is 1.0.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the audio file\n",
    "        \n",
    "    Returns:\n",
    "        Normalized mono audio as float32 numpy array with shape (samples,)\n",
    "    \"\"\"\n",
    "    # Load audio using torchaudio (handles many formats)\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Convert to numpy float32\n",
    "        audio = waveform.numpy().astype(np.float32)\n",
    "        \n",
    "        # Convert to mono by averaging channels if stereo/multi-channel\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = np.mean(audio, axis=0)\n",
    "        else:\n",
    "            audio = audio.squeeze(0)\n",
    "        \n",
    "        # Normalize to peak amplitude of 1.0\n",
    "        peak = np.max(np.abs(audio))\n",
    "        if peak > 0:  # Avoid division by zero for silent audio\n",
    "            audio = audio / peak\n",
    "            \n",
    "        return audio\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load audio file '{file_path}': {str(e)}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ab8760-943c-4714-b133-689acefa5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_pairs(audio_effect, file_count: int = 100) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    # TODO claude: Improve docs\n",
    "    \"\"\"\n",
    "    Read in some files, apply the effect to them\n",
    "\n",
    "    Args:\n",
    "        audio_effect: Function which takes in the clean audio (f32 mono array) and outputs a\n",
    "            processed array of the same size\n",
    "        file_count: How many files to read\n",
    "    \"\"\"\n",
    "    # TODO claude: implement\n",
    "    pass\n",
    "\n",
    "def generate_slices_from_pair(audio_pairs : List[Tuple[np.ndarray, np.ndarray]],\n",
    "                             context_window : int,\n",
    "                             slices_per_pair : int\n",
    "                             ) -> (np.ndarray, np.ndarray):\n",
    "    # TODO claude: Improve docs\n",
    "    \"\"\"\n",
    "    Given reference input/output dry/wet audio pairs,\n",
    "    create some tensors suitable for training our network.\n",
    "    Will generate a bunch of randomly-selected slices from each audio pair.\n",
    "\n",
    "    Args:\n",
    "        audio_pairs: A list of (dry,wet) mono audio arrays. Entire processed songs\n",
    "        context_window: The number of historical input samples the neural network gets\n",
    "        slices_per_pair: The number of example slices we want for each input/output pair\n",
    "\n",
    "    Returns:\n",
    "        An f32 tensor of size (slices_per_pair * len(audio_pairs), context_window * 2 - 1)\n",
    "            The first axis is our \"batch size\", so to speak\n",
    "            The second axis is the `m` input samples concatenated with the `m-1` previous output samples\n",
    "        An f32 tensor of size (slices_per_pair * len(audio_pairs), 1)\n",
    "            This is the next output sample corresponding to each input block\n",
    "    \"\"\"\n",
    "    # TODO claude: implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e0c4c-d3cb-4a22-bf63-6819bd5b13f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
