{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263a750a-808f-4c9c-af75-eac7febd4981",
   "metadata": {},
   "source": [
    "# Training Neural Networks to Emulate Audio Effects\n",
    "\n",
    "I'm gearing up to start a new job at an AI lab, working on RL systems, and I want to do a bit of hands-on NN work to gear up.\n",
    "\n",
    "Let's figure out how to train a model to emulate various audio effects, like distortion units or reverbs or whatever.\n",
    "\n",
    "I haven't read any literature on how to do this, so I'll probably do some stupid stuff here, but I find it's more educational to go in blind sometimes. Basically everything I know about NNs is just from discussions with my friend who works on neural compression codecs.\n",
    "\n",
    "I figure an architecture that might work pretty well is to have the previous `m` input samples, $[i_n, i_{n+1}, ..., i_{n+m-1}, i_{n+m}]$, and the previous `m-1` output samples, $[o_n, o_{n+1}, ..., o_{n+m-1}]$, and train the network to generate the next output sample, $o_{n+m}$\n",
    "\n",
    "```\n",
    "                     .-------.\n",
    "[m prev inputs]----->|  The  |\n",
    "[m-1 prev outputs]-->| Model |-->[next output]\n",
    "                     '-------'\n",
    "```\n",
    "\n",
    "This architecture seems good to me because:\n",
    "\n",
    "1. It's easy to generate examples of input data\n",
    "2. It gives the model enough context to (theoretically) recover information like \"what is the current phase of this oscillator\"\n",
    "\n",
    "\n",
    "As for how to actually represent the audio, some initial thoughts:\n",
    "\n",
    "1. You could just pass in and extract straight up $[-1,1]$ float audio into/out of the model.\n",
    "   1. The model will have to grow hardware to implement a threshold detector ADC or something, which seems kind of wasteful\n",
    "   2. This fails to accurately capture entropy density in human audio perception. A signal with peak amplitude 0.01 is often just as clear to humans as a signal with peak amplitude 1.0. Which brings us to a possible improvement\n",
    "2. Pre- and post-process the audio with some sort of compander, like a $\\mu$-law\n",
    "   1. This will probably help the model maintain perceptual accuracy across wide volume ranges\n",
    "3. Encode a binary representation of the audio. This feels to me like it will probably be difficult for the model to reason about.\n",
    "4. Use some sort of one-hot encoding for the amplitude, probably in combination with a companding algorithm. This might be too expensive on the input side (since we have a lot of inputs), but could work well on the output side. We could have 256 different outputs for 8-bit audio, for example.\n",
    "   1. We could also possibly allow the network to output a continuous \"residual\" value for adding additional accuracy on top of the one-hot value.\n",
    "  \n",
    "For now, let's just try the first thing and see how it goes. I expect it will work OK for well-normalized (loud) audio and poorly for quiet audio.\n",
    "\n",
    "As for our loss function: probably some sort of perceptual similarity metric would be best, but let's start with a super simple metric like squared error in $\\mu$-law space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8cb6e-486e-43a0-9935-4c501337382f",
   "metadata": {},
   "source": [
    "First up, I need a bunch of training data. I took a bunch of songs from my music library and put them in the `examples` folder. (I'm old enought that I have about 75GB of local music files.) \n",
    "\n",
    "When we want to train a new effect, we'll have a bunch of training runs of the form:\n",
    "\n",
    "1. Pick a bunch of random song slices from the library\n",
    "2. Feed the song slices through the reference implementation of the effect (could be software or hardware)\n",
    "3. Train the model to predict the next sample of the effect across all I/O examples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e73757-ce9d-47a5-b8e7-c416baae255a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
