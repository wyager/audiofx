{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263a750a-808f-4c9c-af75-eac7febd4981",
   "metadata": {},
   "source": [
    "# Training Neural Networks to Emulate Audio Effects\n",
    "\n",
    "I'm gearing up to start a new job at an AI lab, working on RL systems, and I want to do a bit of hands-on NN work to gear up.\n",
    "\n",
    "Let's figure out how to train a model to emulate various audio effects, like distortion units or reverbs or whatever. For boilerplate stuff like loading audio data, I'll write a type signature and comment and have Claude fill out the details.\n",
    "\n",
    "I haven't read any literature on how to do stuff like this, so I'll probably do some stupid stuff here, call things by the wrong name, etc., but I find it's often more educational to go in blind sometimes. Basically everything I know about NNs is just from discussions with [my friend who works on neural compression codecs](https://danjacobellis.net).\n",
    "\n",
    "I figure an architecture that might work pretty well is to have the previous `m` input samples, $[i_n, i_{n+1}, ..., i_{n+m-1}, i_{n+m}]$, and the previous `m-1` output samples, $[o_n, o_{n+1}, ..., o_{n+m-1}]$, and train the network to generate the next output sample, $o_{n+m}$\n",
    "\n",
    "```\n",
    "                     .-------.\n",
    "[m prev inputs]----->|  The  |\n",
    "[m-1 prev outputs]-->| Model |-->[next output]\n",
    "                     '-------'\n",
    "```\n",
    "\n",
    "This architecture seems good to me because:\n",
    "\n",
    "1. It's easy to generate examples of input data\n",
    "2. It gives the model enough context to (theoretically) recover information like \"what is the current phase of this oscillator\"\n",
    "\n",
    "\n",
    "As for how to actually represent the audio, some initial thoughts:\n",
    "\n",
    "1. You could just pass in and extract straight up $[-1,1]$ float audio into/out of the model.\n",
    "   1. The model will have to grow hardware to implement a threshold detector ADC or something, which seems kind of wasteful\n",
    "   2. This fails to accurately capture entropy density in human audio perception. A signal with peak amplitude 0.01 is often just as clear to humans as a signal with peak amplitude 1.0. Which brings us to a possible improvement\n",
    "2. Pre- and post-process the audio with some sort of compander, like a $\\mu$-law\n",
    "   1. This will probably help the model maintain perceptual accuracy across wide volume ranges\n",
    "3. Encode a binary representation of the audio. This feels to me like it will probably be difficult for the model to reason about.\n",
    "4. Use some sort of one-hot encoding for the amplitude, probably in combination with a companding algorithm. This might be too expensive on the input side (since we have a lot of inputs), but could work well on the output side. We could have 256 different outputs for 8-bit audio, for example.\n",
    "   1. We could also possibly allow the network to output a continuous \"residual\" value for adding additional accuracy on top of the one-hot value.\n",
    "  \n",
    "For now, let's just try the first thing and see how it goes. I expect it will work OK for well-normalized (loud) audio and poorly for quiet audio.\n",
    "\n",
    "As for our loss function: probably some sort of perceptual similarity metric would be best, but let's start with a super simple metric like squared error in $\\mu$-law space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8cb6e-486e-43a0-9935-4c501337382f",
   "metadata": {},
   "source": [
    "First up, I need a bunch of training data. I took a bunch of songs from my music library and put them in the `examples` folder. (I'm old enought that I have about 75GB of local music files.) \n",
    "\n",
    "When we want to train a new effect, we'll have a bunch of training runs of the form:\n",
    "\n",
    "1. Pick a bunch of random song slices from the library\n",
    "2. Feed the song slices through the reference implementation of the effect (could be software or hardware)\n",
    "3. Train the model to predict the next sample of the effect across all I/O examples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e73757-ce9d-47a5-b8e7-c416baae255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def list_audio_files(examples_dir: str = \"examples\") -> List[str]:\n",
    "    \"\"\"\n",
    "    List all audio files in the examples directory.\n",
    "    \n",
    "    Args:\n",
    "        examples_dir: Path to the examples directory\n",
    "        \n",
    "    Returns:\n",
    "        List of audio file paths\n",
    "    \"\"\"\n",
    "    audio_extensions = {'.mp3', '.m4a', '.wav', '.flac', '.aac', '.ogg', '.mp4'}\n",
    "    audio_files = []\n",
    "    \n",
    "    examples_path = Path(examples_dir)\n",
    "    if not examples_path.exists():\n",
    "        raise FileNotFoundError(f\"Examples directory '{examples_dir}' not found\")\n",
    "    \n",
    "    for file_path in examples_path.rglob('*'):\n",
    "        if file_path.is_file() and file_path.suffix.lower() in audio_extensions:\n",
    "            audio_files.append(str(file_path))\n",
    "    \n",
    "    return sorted(audio_files)\n",
    "\n",
    "def read_audio_normalized(file_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read an audio file, convert to mono, and normalize so peak absolute value is 1.0.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the audio file\n",
    "        \n",
    "    Returns:\n",
    "        Normalized mono audio as float32 numpy array with shape (samples,)\n",
    "    \"\"\"\n",
    "    # Load audio using torchaudio (handles many formats)\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Convert to numpy float32\n",
    "        audio = waveform.numpy().astype(np.float32)\n",
    "        \n",
    "        # Convert to mono by averaging channels if stereo/multi-channel\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = np.mean(audio, axis=0)\n",
    "        else:\n",
    "            audio = audio.squeeze(0)\n",
    "        \n",
    "        # Normalize to peak amplitude of 1.0\n",
    "        peak = np.max(np.abs(audio))\n",
    "        if peak > 0:  # Avoid division by zero for silent audio\n",
    "            audio = audio / peak\n",
    "            \n",
    "        return audio\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load audio file '{file_path}': {str(e)}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056538a-a779-42bf-b9ec-d16581055df0",
   "metadata": {},
   "source": [
    "OK, now let's write some code to generate input/output data for training the network.\n",
    "\n",
    "Our input tensor will be of shape `(batch size, m input samples + m-1 output samples)` and our output tensor will\n",
    "be of shape `(batch size, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ab8760-943c-4714-b133-689acefa5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO WYAGER: check Claude's work\n",
    "def generate_audio_pairs(audio_effect, file_count: int = 64) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    TODO CLAUDE: Read in random audio files from examples directory and apply the effect to them,\n",
    "    creating pairs of (dry, wet) audio for training.\n",
    "\n",
    "    Args:\n",
    "        audio_effect: Function which takes in clean audio (f32 mono array) and outputs a\n",
    "            processed array of the same size\n",
    "        file_count: How many files to read and process\n",
    "\n",
    "    Returns:\n",
    "        List of (dry_audio, wet_audio) pairs as numpy arrays\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If not enough audio files are available\n",
    "        RuntimeError: If audio processing fails\n",
    "    \"\"\"\n",
    "    audio_files = list_audio_files()\n",
    "    \n",
    "    if len(audio_files) < file_count:\n",
    "        raise ValueError(f\"Not enough audio files: found {len(audio_files)}, need {file_count}\")\n",
    "    \n",
    "    # Sample random files without replacement\n",
    "    selected_files = random.sample(audio_files, file_count)\n",
    "    \n",
    "    audio_pairs = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in selected_files:\n",
    "        try:\n",
    "            # Read and normalize the dry audio\n",
    "            dry_audio = read_audio_normalized(file_path)\n",
    "            \n",
    "            # Apply the effect to get wet audio\n",
    "            wet_audio = audio_effect(dry_audio)\n",
    "            \n",
    "            # Validate that effect preserved array shape\n",
    "            if wet_audio.shape != dry_audio.shape:\n",
    "                raise RuntimeError(f\"Audio effect changed shape: {dry_audio.shape} -> {wet_audio.shape}\")\n",
    "            \n",
    "            audio_pairs.append((dry_audio, wet_audio))\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path, str(e)))\n",
    "            print(f\"Warning: Failed to process {file_path}: {e}\")\n",
    "    \n",
    "    if failed_files and len(audio_pairs) == 0:\n",
    "        raise RuntimeError(f\"All audio files failed to process: {failed_files}\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(audio_pairs)} audio files ({len(failed_files)} failed)\")\n",
    "    return audio_pairs\n",
    "\n",
    "# TODO WYAGER: check Claude's work  \n",
    "def generate_slices_from_pairs(audio_pairs: List[Tuple[np.ndarray, np.ndarray]],\n",
    "                               context_window: int,\n",
    "                               slices_per_pair: int\n",
    "                               ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    TODO CLAUDE: Given reference input/output dry/wet audio pairs,\n",
    "    create numpy arrays suitable for training the neural network.\n",
    "    Will generate randomly-selected slices from each audio pair.\n",
    "\n",
    "    Args:\n",
    "        audio_pairs: A list of (dry,wet) mono audio arrays (entire processed songs)\n",
    "        context_window: The number of historical input samples the neural network gets\n",
    "        slices_per_pair: The number of example slices we want for each input/output pair\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (input_features, target_outputs):\n",
    "        - input_features: f32 array of shape (total_slices, context_window * 2 - 1)\n",
    "            Each row contains `context_window` input samples concatenated with \n",
    "            `context_window - 1` previous output samples\n",
    "        - target_outputs: f32 array of shape (total_slices, 1)\n",
    "            The next output sample corresponding to each input block\n",
    "            \n",
    "    Raises:\n",
    "        ValueError: If audio pairs are too short for the context window\n",
    "    \"\"\"\n",
    "    if not audio_pairs:\n",
    "        raise ValueError(\"No audio pairs provided\")\n",
    "    \n",
    "    total_slices = len(audio_pairs) * slices_per_pair\n",
    "    feature_size = context_window * 2 - 1\n",
    "    \n",
    "    # Pre-allocate output arrays\n",
    "    input_features = np.zeros((total_slices, feature_size), dtype=np.float32)\n",
    "    target_outputs = np.zeros((total_slices, 1), dtype=np.float32)\n",
    "    \n",
    "    slice_idx = 0\n",
    "    \n",
    "    for dry_audio, wet_audio in audio_pairs:\n",
    "        # Check minimum length requirement\n",
    "        min_length = context_window * 2  # Need context_window for input + context_window for output history + 1 for target\n",
    "        if len(dry_audio) < min_length:\n",
    "            print(f\"Warning: Audio pair too short ({len(dry_audio)} samples), need at least {min_length}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Generate random slice positions for this pair\n",
    "        # Valid range: from context_window to len-1 (need context_window previous samples)\n",
    "        max_start = len(dry_audio) - context_window\n",
    "        slice_positions = np.random.randint(context_window, max_start + 1, size=slices_per_pair)\n",
    "        \n",
    "        for pos in slice_positions:\n",
    "            # Extract input context: previous context_window input samples\n",
    "            input_context = dry_audio[pos - context_window:pos]\n",
    "            \n",
    "            # Extract output context: previous context_window-1 output samples  \n",
    "            output_context = wet_audio[pos - context_window:pos - 1]\n",
    "            \n",
    "            # Combine input and output contexts\n",
    "            input_features[slice_idx] = np.concatenate([input_context, output_context])\n",
    "            \n",
    "            # Target is the next output sample\n",
    "            target_outputs[slice_idx, 0] = wet_audio[pos]\n",
    "            \n",
    "            slice_idx += 1\n",
    "    \n",
    "    # Trim arrays if some pairs were skipped\n",
    "    if slice_idx < total_slices:\n",
    "        input_features = input_features[:slice_idx]\n",
    "        target_outputs = target_outputs[:slice_idx]\n",
    "        print(f\"Generated {slice_idx} slices (some audio pairs were too short)\")\n",
    "    else:\n",
    "        print(f\"Generated {slice_idx} slices from {len(audio_pairs)} audio pairs\")\n",
    "    \n",
    "    return input_features, target_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d7846-e051-4d54-9e10-53212669ef8c",
   "metadata": {},
   "source": [
    "Let's try this with a \"passthrough\" audio effect that does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02b6e12-2c77-45eb-9a0d-1362c546bc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 64 audio files (0 failed)\n",
      "Generated 16384 slices from 64 audio pairs\n"
     ]
    }
   ],
   "source": [
    "context_window = 128\n",
    "slices_per_pair = 256\n",
    "file_count = 64\n",
    "\n",
    "effect = lambda x : x # Do nothing to the audio\n",
    "passthru_data = \\\n",
    "    generate_slices_from_pairs(generate_audio_pairs(effect, \n",
    "                                                    file_count = file_count),\n",
    "                               context_window, \n",
    "                               slices_per_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b5bb0-bdd1-410f-ad38-999fd7ef9cbb",
   "metadata": {},
   "source": [
    "Now for our actual training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26a7d6b8-fb07-4570-a37b-e04f41a6f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "inner_layer_size = 1024\n",
    "num_inner_layers = 5\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, input_layer, input_biases, inner_layers, inner_biases, output_layer, output_bias):\n",
    "        self.input_layer, self.input_biases, self.inner_layers, self.inner_biases, self.output_layer, self.output_bias = \\\n",
    "            input_layer, input_biases, inner_layers, inner_biases, output_layer, output_bias\n",
    "    # TODO claude: Add a type here\n",
    "    def eval(self, inputs):\n",
    "        # Run the model forward\n",
    "        vec = torch.relu(inputs @ self.input_layer + self.input_biases)\n",
    "        for i,layer in enumerate(self.inner_layers):\n",
    "            bias = self.inner_biases[i]\n",
    "            vec = torch.relu(vec @ layer + bias)\n",
    "        vec = torch.relu(vec @ self.output_layer + self.output_bias)\n",
    "        return vec\n",
    "    def cpu(self):\n",
    "        return Model(\n",
    "            self.input_layer.cpu(), \n",
    "            self.input_biases.cpu(), \n",
    "            self.inner_layers.cpu(), \n",
    "            self.inner_biases.cpu(), \n",
    "            self.output_layer.cpu(), \n",
    "            self.output_bias.cpu())\n",
    "\n",
    "def train(examples):\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (GPU) device\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU device\")\n",
    "    losses = []\n",
    "    inputs, outputs = examples\n",
    "    inputs, outputs = torch.tensor(inputs).to(device), torch.tensor(outputs).to(device)\n",
    "    (samp_count, _) = inputs.shape\n",
    "    # Input layer maps from the input context to our inner layers\n",
    "    input_layer = torch.randn((context_window * 2 - 1, inner_layer_size), requires_grad = True, device=device)\n",
    "    input_biases = torch.randn(inner_layer_size, requires_grad = True, device=device)\n",
    "    # Inner layers do whatever\n",
    "    inner_layers = [torch.randn((inner_layer_size, inner_layer_size), requires_grad=True, device=device) for _ in range(num_inner_layers)]\n",
    "    inner_biases = [torch.randn(inner_layer_size, requires_grad = True, device=device) for _ in range(num_inner_layers)]\n",
    "    # Maps from our last layer to the next audio output value\n",
    "    output_layer = torch.randn((inner_layer_size, 1), requires_grad = True, device=device)\n",
    "    output_bias = torch.randn(1, requires_grad = True, device=device)\n",
    "    model = Model(input_layer, input_biases, inner_layers, inner_biases, output_layer, output_bias)\n",
    "    ins = torch.tensor(inputs).to(device)\n",
    "    outs = torch.tensor(outputs).to(device)\n",
    "    # Automatically adjusts learning rate for each param, uses momentum, etc.\n",
    "    optimizer = torch.optim.Adam([input_layer, input_biases] + inner_layers + inner_biases + [output_layer, output_bias], lr=0.01)\n",
    "    # Run a bunch of optimization steps\n",
    "    for step in range(100):\n",
    "        # Reset the gradients on each leaf node\n",
    "        optimizer.zero_grad()\n",
    "        # Run the model forward\n",
    "        vec = model.eval(inputs)\n",
    "        # torch.relu(inputs @ input_layer + input_bias)\n",
    "        # for i,layer in enumerate(inner_layers):\n",
    "        #     bias = inner_biases[i]\n",
    "        #     vec = torch.relu(vec @ layer + bias)\n",
    "        # vec = torch.relu(vec @ output_layer + output_bias)\n",
    "        # Let's just try it with non-companded error. It should perform badly on low-magnitude signals\n",
    "        error = vec - outputs\n",
    "        # Squared error\n",
    "        loss = (error ** 2).sum() / samp_count\n",
    "        loss.backward()\n",
    "        losses.append(loss.cpu().item())\n",
    "        if step % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "        # Let the optimizer adjust the params\n",
    "        # Conceptually it's basically doing param -= param.grad * 0.01 for each param\n",
    "        # But it's doing stuff with momentum, normalization, etc. to make it converge faster\n",
    "        optimizer.step()\n",
    "    return model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90c13f15-56f6-4bb4-bc2d-502d1774d82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassthru_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 83\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Let the optimizer adjust the params\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Conceptually it's basically doing param -= param.grad * 0.01 for each param\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# But it's doing stuff with momentum, normalization, etc. to make it converge faster\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 25\u001b[0m, in \u001b[0;36mModel.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_biases\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_layers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m(), \n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_biases\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_bias\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "train(passthru_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc986b2-b707-43af-8534-dc608d1b7694",
   "metadata": {},
   "source": [
    "OK, cool, that converged pretty much instantly. Let's sanity check that this sounds OK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
